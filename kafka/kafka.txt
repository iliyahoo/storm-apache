systemctl stop firewalld
systemctl disable firewalld

# install java
yum install -y java-1.8.0-openjdk

wget http://apache.mivzakim.net/kafka/0.8.2.1/kafka_2.9.1-0.8.2.1.tgz
tar -xvf kafka_2.9.1-0.8.2.1.tgz
mv kafka_2.9.1-0.8.2.1 /srv/
ln -s /srv/kafka_2.9.1-0.8.2.1 /srv/kafka
cd /srv/kafka

# on one of the zookeeper's node create the kafka's root path:
zkCli.sh -server 127.0.0.1:2181
create /kafka ''

# edit config/server.properties
sed -i "s|^\(zookeeper.connect=\).*|\1172.28.128.10:2181,172.28.128.11:2181,172.28.128.12:2181/kafka|g" /srv/kafka/config/server.properties
id=$(echo ${HOSTNAME} | tr -d '[a-z-.]')
sed -i "s/\(broker.id=\).*/\1$id/g" /srv/kafka/config/server.properties
echo "delete.topic.enable=true" >> /srv/kafka/config/server.properties

echo "PATH=$PATH:/srv/kafka/bin" >> ~/.bash_profile
. ~/.bash_profile

kafka-server-start.sh config/server.properties

# supervisord
wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum -y localinstall epel-release-latest-7.noarch.rpm

yum -y install supervisor
systemctl enable supervisord.service
systemctl start supervisord.service

cat << EOF > /etc/supervisord.d/kafka.ini
[program:kafka]
command=/srv/kafka/bin/kafka-server-start.sh /srv/kafka/config/server.properties
autostart=true
startsecs=10
startretries=9
logfile_maxbytes=20MB
logfile_backups=10
stderr_logfile=/var/log/supervisor-kafka.err.log
stdout_logfile=/var/log/supervisor-kafka.out.log
EOF

supervisorctl reload

# some commands for testing
kafka-console-consumer.sh --zookeeper zookeeper1:2181,zookeeper2:2181,zookeeper3:2181/kafka --topic topic-1 --from-beginning

kafka-console-producer.sh --broker-list 172.28.128.16:9092,172.28.128.17:9092,172.28.128.18:9092 --topic topic-1

kafka-topics.sh --zookeeper 172.28.128.10:2181,172.28.128.11:2181,172.28.128.12:2181/kafka --describe --topic topic-1

# validate using the consumer offset checker
# create node on zookeeper
create /kafka/consumers/<consumer_group>/owners ''
kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper 172.28.128.10:2181,172.28.128.11:2181,172.28.128.12:2181/kafka --broker-info --group <consumer_group> --topic topic-1

# every replica in ISR has all messages that are committed. A replica will be dropped out of ISR if it diverges from the leader. This is controlled by two parameters: replica.lag.time.max.ms and replica.lag.max.messages. The former is typically set to a value that reliably detects the failure of a broker. We have a min fetch rate JMX in the broker. If that rate is n, set the former to a value larger than 1/n * 1000. The latter is typically set to the observed max lag (a JMX bean) in the follower. Note that if replica.lag.max.messages is too large, it can increase the time to commit a message. If latency becomes a problem, you can increase the number of partitions in a topic. If a replica constantly drops out of and rejoins isr, you may need to increase replica.lag.max.messages. If a replica stays out of ISR for a long time, it may indicate that the follower is not able to fetch data as fast as data is accumulated at the leader. You can increase the follower's fetch throughput by setting a larger value for num.replica.fetchers

# replica.fetch.max.bytes == message.max.bytes

# ISR drops batch messages if they more than 4000 messages behind the leader
# so split them

# out of memory
replica.message.max.bytes * N of partitions the broker replicates
fetch.message.max.bytes * N of partitions the consumer reads

# Synchronous replication
Our synchronous replication follows the typical primary-backup approach. Each partition has n replicas and can tolerate n-1 replica failures. One of the replicas is elected as the leader and the rest of the replicas are followers. The leader maintains a set of in-sync replicas (ISR): the set of replicas that have fully caught up with the leader. For each partition, we store in Zookeeper the current leader and the current ISR.
Each replica stores messages in a local log and maintains a few important offset positions in the log (depicted in Figure 1). The log end offset (LEO) represents the tail of the log. The high watermark (HW) is the offset of the last committed message. Each log is periodically synced to disks. Data before the flushed offset is guaranteed to be persisted on disks. As we will see, the flush offset can be before or after HW.

# kafka replication
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication

# benchmarking kafka
http://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines
